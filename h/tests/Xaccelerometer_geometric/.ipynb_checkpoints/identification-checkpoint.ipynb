{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETER IDENTIFICATION NOTEBOOK\n",
    "\n",
    "In this notebook we leverage the pretrained surrogate models to identify fabrication uncertainties in MEMS accelerometers. We start from noisy signals. The noise is an additive white noise, manually added to the data to emulate experimental data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# CUQI library imports\n",
    "import cuqi\n",
    "from cuqi.model import Model\n",
    "from cuqi.distribution import Gaussian, Uniform, JointDistribution\n",
    "from cuqi.sampler import MH, NUTS\n",
    "from cuqi.geometry import Continuous1D, Discrete\n",
    "\n",
    "# Local module imports\n",
    "sys.path.append('../../src/SurrogateModeling')\n",
    "sys.path.append('../../src/InverseProblems')\n",
    "sys.path.append('../../src/utils')\n",
    "from utils import * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide sample to use for experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 10 # Which sample of the training set do wou want to take as experimental input?\n",
    "OUTPUT_FILENAME = \"./samples/sample_\"+str(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Bayesian Identification Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate Model Configurations\n",
    "CONFIGURATION_I = './config_I.json'\n",
    "CONFIGURATION_II = './config_II.json'\n",
    "\n",
    "# Markov Chain Monte Carlo (MCMC) Configuration\n",
    "MCMC_SETTINGS = {\n",
    "    'parameter_start_points': [\n",
    "        np.array([0.3, 0.0, 30.0]),\n",
    "        np.array([0.4, 0.25, 30.0]),\n",
    "        np.array([0.2, 0.25, 30.0]),\n",
    "        np.array([0.4, -0.25, 30.0]),\n",
    "        np.array([0.2, -0.25, 30.0])\n",
    "    ],\n",
    "    'bounds': ([0.1, -0.5, 29.0], [0.5, 0.5, 31.0]),\n",
    "    'N': int(6e3),   # Total number of samples\n",
    "    'Nb': int(1e3),  # Number of burn-in samples\n",
    "    'Nt': 5,         # Number of chains\n",
    "}\n",
    "\n",
    "# Noise Configuration\n",
    "NOISE_PARAMS = {\n",
    "    'noise_factor': 1e-6 * 1000,\n",
    "    'B': np.sqrt(200),\n",
    "    'S': 5,\n",
    "}\n",
    "\n",
    "noise = (NOISE_PARAMS['noise_factor']*NOISE_PARAMS['B']*NOISE_PARAMS['S'])**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Surrogate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for both configurations\n",
    "data_processor_I = preprocessing(CONFIGURATION_I)\n",
    "data_processor_II = preprocessing(CONFIGURATION_II)\n",
    "\n",
    "# Initialize and load models for both configurations\n",
    "model_I = NN_Model()\n",
    "model_I.load_model(data_processor_I.config['MODEL_PATH'])\n",
    "\n",
    "model_II = NN_Model()\n",
    "model_II.load_model(data_processor_II.config['MODEL_PATH'])  # Ensure this uses data_processor_II\n",
    "\n",
    "# Define forward and gradient functions for the first configuration\n",
    "forward_model = create_forward_model_function(data_processor_I, model_I)\n",
    "gradient_model = create_gradient_function(data_processor_I, model_I)\n",
    "\n",
    "# Create a CUQI model using the forward model and gradient functions\n",
    "cuqi_model = Model(forward=forward_model, \n",
    "                   jacobian=gradient_model,\n",
    "                   range_geometry=Continuous1D(len(data_processor_I.time)),\n",
    "                   domain_geometry=Discrete([\"Overetch\", \"Offset\", \"Thickness\"]))\n",
    "\n",
    "# Extract test data for visualization or further processing\n",
    "X_values, y_values = data_processor_I.X_test, data_processor_I.y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a true sample for testing\n",
    "x_true, y_true = X_values[sample], y_values[sample]\n",
    "\n",
    "# Generate observed data by adding Gaussian noise to the true data\n",
    "y_observed = Gaussian(mean=y_true, cov=noise * np.eye(len(data_processor_I.time))).sample()\n",
    "\n",
    "# Define the prior distribution for the input parameters\n",
    "# Assuming uniform distributions over specified ranges for each parameter\n",
    "x_distribution = Uniform(low=np.array([0.1, -0.5, 29.0]), high=np.array([0.5, 0.5, 31.0]))\n",
    "\n",
    "# Define the likelihood distribution for the output\n",
    "# Gaussian distribution centered around the model's predictions with specified noise\n",
    "y_distribution = Gaussian(mean=cuqi_model(x_distribution), cov=noise * np.eye(len(data_processor_I.time)))\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform least square optimization for each starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Params:  [ 0.345976 -0.423282 30.139451]\n",
      "Optimized Params 1: [ 0.34595859 -0.42218437 30.23211804]\n",
      "Optimized Params 2: [ 0.34565114 -0.42242781 30.19061334]\n",
      "Optimized Params 3: [ 0.34623373 -0.42204428 30.26282401]\n",
      "Optimized Params 4: [ 0.34578237 -0.42233314 30.2075441 ]\n",
      "Optimized Params 5: [ 0.34566566 -0.42240065 30.19388258]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to hold the optimized parameters for each start point\n",
    "initial_guesses = []\n",
    "\n",
    "# Display the true parameters for reference\n",
    "print(\"Real Params: \", x_true)\n",
    "\n",
    "# Iterate over each parameter start point to perform optimization\n",
    "for index, start_point in enumerate(MCMC_SETTINGS['parameter_start_points']):\n",
    "    # Perform least squares optimization given the observed data, forward model,\n",
    "    # start point, and parameter bounds\n",
    "    optimized_params, covariance_matrix = least_squares_optimization(\n",
    "        y_observed=y_observed, \n",
    "        forward_model=forward_model, \n",
    "        start_point=start_point, \n",
    "        bounds=MCMC_SETTINGS['bounds']\n",
    "    )\n",
    "\n",
    "    # Append the optimized parameters to the list of initial guesses\n",
    "    initial_guesses.append(optimized_params)\n",
    "\n",
    "    # Print the optimized parameters for this iteration\n",
    "    print(f\"Optimized Params {index + 1}: {optimized_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do Metropolis Hastings Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 6000 / 6000\n",
      "\n",
      "Average acceptance rate: 0.268 MCMC scale: 0.12332527943465017 \n",
      "\n",
      "Effective Sample Size:  [356.22446203 424.96890589 380.86544944] \n",
      "\n",
      "Sample 6000 / 6000\n",
      "\n",
      "Average acceptance rate: 0.2723333333333333 MCMC scale: 0.12611279524644892 \n",
      "\n",
      "Effective Sample Size:  [430.7681572  332.30035237 321.98710538] \n",
      "\n",
      "Sample 6000 / 6000\n",
      "\n",
      "Average acceptance rate: 0.2695 MCMC scale: 0.12231800570061512 \n",
      "\n",
      "Effective Sample Size:  [310.31984717 444.00951804 329.39206245] \n",
      "\n",
      "Sample 2760 / 6000"
     ]
    }
   ],
   "source": [
    "# Define the covariance matrix for the Markov chain sampling, scaled by the noise factor\n",
    "cov_matrix = covariance_matrix\n",
    "\n",
    "# Set up the posterior distribution by combining the prior (x_distribution)\n",
    "# and the likelihood (y_distribution) given the observed data (y_observed)\n",
    "posterior = JointDistribution(x_distribution, y_distribution)(y_distribution=y_observed)\n",
    "\n",
    "# Initialize a list to hold the Markov chain samples for each initial guess\n",
    "samples_mh = []\n",
    "\n",
    "# Iterate over each initial guess to set up and run the Markov chain sampler\n",
    "for index, initial_guess in enumerate(initial_guesses):\n",
    "    # Set up the Markov chain sampler with the posterior, covariance matrix, and initial guessi\n",
    "    proposal = Gaussian(mean=np.zeros(3), cov=cov_matrix)\n",
    "    mc_sampler =  MH(target=posterior, proposal=proposal, x0=initial_guess)\n",
    "    samples = mc_sampler.sample_adapt(MCMC_SETTINGS['N'], 0)  # Assuming 'N' is the number of samples desired\n",
    "    \n",
    "    # Append the samples to the list\n",
    "    samples_mh.append(samples)\n",
    "    \n",
    "    # Compute and print the Effective Sample Size (ESS) of the first set of samples\n",
    "    print(\"Effective Sample Size: \", samples_mh[index].burnthin(MCMC_SETTINGS['Nb'],MCMC_SETTINGS['Nt']).compute_ess(),\"\\n\")\n",
    "\n",
    "# Computing diagnostics and collecting results\n",
    "print(\"Rhat: \", samples_mh[0].compute_rhat(samples_mh[1:]))\n",
    "\n",
    "# Save the numpy array to a file\n",
    "np.save(OUTPUT_FILENAME, samples_mh[0].samples)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the trace plot of one chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples_mh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot trace of the first set of samples\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msamples_mh\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mburnthin(MCMC_SETTINGS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNb\u001b[39m\u001b[38;5;124m'\u001b[39m],MCMC_SETTINGS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNt\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mplot_trace()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples_mh' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot trace of the first set of samples\n",
    "samples_mh[0].burnthin(MCMC_SETTINGS['Nb'],MCMC_SETTINGS['Nt']).plot_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the prediction of the mean parameter combination and the experimental signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and data collection\n",
    "plot_results(data_processor_I.time, forward_model(x_true), y_observed, forward_model, samples_mh[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the parameters distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    plot_parameter_distribution(samples_mh[0].samples[j, :], x_true[j], ['Overetch', 'Offset', 'Thickness'][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing Plots\n",
    "\n",
    "This section generates the plots in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotsPaper import *\n",
    "samples = np.load(OUTPUT_FILENAME+'.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histograms of the geometric parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(samples, x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_scatter(samples, x_true, sigma_values=(0.2, 0.5))  # Adjust sigma values as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_true = data_processor_II.y_test[sample]\n",
    "\n",
    "plot_sensitivity_histogram(samples, S_true, model_II, data_processor_II.scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
